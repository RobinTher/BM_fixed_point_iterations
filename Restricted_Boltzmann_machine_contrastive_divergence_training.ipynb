{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import gc\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spins(P):\n",
    "    \n",
    "    spins_base = np.full((P, 2), np.array([-1., 1.]))\n",
    "    \n",
    "    spins = np.reshape(np.array(np.meshgrid(*spins_base)).T, newshape = (-1,) + (P, 1))\n",
    "    \n",
    "    # spins_T = jnp.transpose(spins, axes = (0, 2, 1))\n",
    "    \n",
    "    # return spins_T, spins\n",
    "    return spins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_free_energy_difference(student_A, student_B, beta, P, device):\n",
    "    \n",
    "    xi_A = student_A.xi\n",
    "    \n",
    "    xi_B = student_B.xi\n",
    "    \n",
    "    spins = torch.from_numpy(np.float32(init_spins(P))).to(device)\n",
    "    \n",
    "    H = 1/2 * beta**2 * torch.sum((xi_A @ spins)**2, dim = 1)\n",
    "    c = torch.max(H)\n",
    "    print(\"Epsilon: %.2f\" % torch.max(beta * xi_A @ spins))\n",
    "    f_A = c + torch.log(torch.mean(torch.exp(H - c)))\n",
    "    \n",
    "    H = 1/2 * beta**2 * torch.sum((xi_B @ spins)**2, dim = 1)\n",
    "    c = torch.max(H)\n",
    "    print(\"Epsilon: %.2f\" % torch.max(beta * xi_B @ spins))\n",
    "    f_B = c + torch.log(torch.mean(torch.exp(H - c)))\n",
    "    \n",
    "    f_difference_1 = f_B - f_A\n",
    "    \n",
    "    H = torch.sum(logcosh(beta * xi_A @ spins), dim = 1)\n",
    "    c = torch.max(H)\n",
    "    f_A = c + torch.log(torch.mean(torch.exp(H - c)))\n",
    "    \n",
    "    H = torch.sum(logcosh(beta * xi_B @ spins), dim = 1)\n",
    "    c = torch.max(H)\n",
    "    f_B = c + torch.log(torch.mean(torch.exp(H - c)))\n",
    "    \n",
    "    f_difference_2 = f_B - f_A\n",
    "    \n",
    "    # 22.269562\n",
    "    # 0.23609748\n",
    "    # 22.390316\n",
    "    # 0.22040498\n",
    "    # -0.3633461\n",
    "    \n",
    "    return f_difference_1, f_difference_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logcosh(x):\n",
    "    c = torch.maximum(-x, x)\n",
    "    \n",
    "    return c + torch.log1p(torch.exp(-2*c)) - torch.log(torch.tensor(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_logcosh(x):\n",
    "    c = jnp.maximum(-x, x)\n",
    "    \n",
    "    return c + jnp.log1p(jnp.exp(-2*c)) - jnp.log(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rademacher(prob, generator : torch.Generator):\n",
    "    return 2*torch.bernoulli(prob, generator = generator)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_rademacher(prob, key : int):\n",
    "    return 2*jax.random.bernoulli(key, prob)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(torch.nn.Module):\n",
    "    def __init__(self, N : int, P : int, standard_deviation : float, device : torch.device, random_number_generator : torch.Generator):\n",
    "        super(RBM, self).__init__()\n",
    "        \n",
    "        self.N = N\n",
    "        self.P = P\n",
    "        \n",
    "        self.training_device = device\n",
    "        \n",
    "        self.random_number_generator = random_number_generator\n",
    "        \n",
    "        self.xi = torch.nn.Parameter(torch.zeros((N, P), device = device), requires_grad = False)\n",
    "        \n",
    "        self.initialize_weights(standard_deviation)\n",
    "        \n",
    "        self.descent_vector = torch.nn.Parameter(torch.zeros((N, P), device = device), requires_grad = False)\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def initialize_weights(self, standard_deviation : float):\n",
    "        xi = torch.randn((self.N, self.P), device = self.training_device, generator = self.random_number_generator)\n",
    "        \n",
    "        xi.copy_((xi - torch.flip(xi, dims = (0,)))/torch.sqrt(torch.tensor(2)))\n",
    "    \n",
    "        C = (torch.transpose(xi, 0, 1) @ xi) / self.N\n",
    "        \n",
    "        L = torch.linalg.cholesky(C)\n",
    "        xi.copy_(torch.linalg.solve_triangular(L, xi.T, upper = False).T)\n",
    "        \n",
    "        self.xi.copy_(standard_deviation * xi)\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def sample_hidden_given_visible(self, tau, sigma, beta : float):\n",
    "        P_tau_given_sigma = torch.sigmoid(beta * sigma @ self.xi)\n",
    "        tau.copy_(rademacher(P_tau_given_sigma, generator = self.random_number_generator))\n",
    "        \n",
    "        return P_tau_given_sigma\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def sample_visible_given_hidden(self, sigma, tau, beta : float):\n",
    "        P_sigma_given_tau = torch.sigmoid(beta * tau @ torch.transpose(self.xi, 0, 1))\n",
    "        sigma.copy_(rademacher(P_sigma_given_tau, generator = self.random_number_generator))\n",
    "        \n",
    "        return P_sigma_given_tau\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def sample_visible_and_hidden(self, sigma, tau, beta : float, number_sampling_steps : int,\n",
    "                                  number_monitored_sampling_steps : int, calculate_loss : bool):\n",
    "        \n",
    "        f_0 = torch.tensor(0.)\n",
    "        f = torch.tensor(0.)\n",
    "        \n",
    "        P_sigma_given_tau = torch.zeros_like(sigma, device = self.training_device)\n",
    "        P_tau_given_sigma = torch.zeros_like(tau, device = self.training_device)\n",
    "        \n",
    "        if number_monitored_sampling_steps != 0:\n",
    "            f_0 = torch.mean(self.free_entropy(sigma, beta))\n",
    "            print(\"Step [{}/{}], free entropy: {:.4f}\".format(0, number_sampling_steps, f_0))\n",
    "        \n",
    "        elif calculate_loss:\n",
    "            f_0 = torch.mean(self.free_entropy(sigma, beta))\n",
    "        \n",
    "        for sampling_step in range(1, number_sampling_steps + 1):\n",
    "            if number_monitored_sampling_steps != 0:\n",
    "                monitor_this_step = sampling_step % (number_sampling_steps // number_monitored_sampling_steps) == 0\n",
    "            else:\n",
    "                monitor_this_step = False\n",
    "            \n",
    "            # if anneal:\n",
    "                # beta_cur = beta * sampling_step / number_sampling_steps\n",
    "            # else:\n",
    "                # beta_cur = beta\n",
    "            \n",
    "            P_sigma_given_tau = self.sample_visible_given_hidden(sigma, tau, beta)\n",
    "            # print(P_sigma_given_tau)\n",
    "            \n",
    "            P_tau_given_sigma = self.sample_hidden_given_visible(tau, sigma, beta)\n",
    "            \n",
    "            if monitor_this_step:\n",
    "                f = torch.mean(self.free_entropy(sigma, beta))\n",
    "                print(\"Step [{}/{}], free entropy: {:.4f}\".format(sampling_step, number_sampling_steps, f))\n",
    "        \n",
    "        if calculate_loss:\n",
    "            f = torch.mean(self.free_entropy(sigma, beta))\n",
    "            loss = f - f_0\n",
    "        else:\n",
    "            loss = torch.tensor(0.)\n",
    "        \n",
    "        return P_sigma_given_tau, loss\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def sample_visible(self, sigma, beta : float, number_sampling_steps : int,\n",
    "                       number_monitored_sampling_steps : int):\n",
    "        \n",
    "        tau = torch.zeros((len(sigma), self.P), device = self.training_device)\n",
    "        \n",
    "        P_tau_given_sigma = self.sample_hidden_given_visible(tau, sigma, beta)\n",
    "        \n",
    "        P_sigma_given_tau, _ = self.sample_visible_and_hidden(sigma, tau, beta, number_sampling_steps,\n",
    "                                                              number_monitored_sampling_steps = number_monitored_sampling_steps,\n",
    "                                                              calculate_loss = False)\n",
    "        \n",
    "        del tau\n",
    "        \n",
    "        return P_sigma_given_tau\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def free_entropy(self, sigma, beta : float):\n",
    "        f = torch.sum(logcosh(beta * sigma @ self.xi), dim = 1)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def contrastive_divergence(self, sigma, beta : float, number_sampling_steps : int,\n",
    "                               monitor_sampling : bool, calculate_loss : bool):\n",
    "        if monitor_sampling:\n",
    "            number_monitored_sampling_steps = number_sampling_steps\n",
    "        else:\n",
    "            number_monitored_sampling_steps = 0\n",
    "        \n",
    "        tau = torch.zeros((len(sigma), self.P), device = self.training_device)\n",
    "        \n",
    "        P_tau_given_sigma = self.sample_hidden_given_visible(tau, sigma, beta)\n",
    "        \n",
    "        positive_gradient = torch.mean(torch.reshape(sigma, (-1, self.N, 1)) @ torch.reshape(tau, (-1, 1, self.P)), dim = 0)\n",
    "        \n",
    "        _, loss = self.sample_visible_and_hidden(sigma, tau, beta, number_sampling_steps,\n",
    "                                                 number_monitored_sampling_steps = number_monitored_sampling_steps,\n",
    "                                                 calculate_loss = calculate_loss)\n",
    "        \n",
    "        negative_gradient = torch.mean(torch.reshape(sigma, (-1, self.N, 1)) @ torch.reshape(tau, (-1, 1, self.P)), dim = 0)\n",
    "        \n",
    "        del tau\n",
    "        \n",
    "        gradient = positive_gradient - negative_gradient\n",
    "        \n",
    "        reconstruction_error = torch.sum(gradient**2)\n",
    "        \n",
    "        gradient = beta*gradient\n",
    "        \n",
    "        return loss, reconstruction_error, gradient\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def weights_update(self, sigma, beta : float, alpha : float, learning_rate : float,\n",
    "                       decay_rate : float, momentum : float, number_sampling_steps : int,\n",
    "                       monitor_sampling : bool, calculate_loss : bool):\n",
    "        \n",
    "        loss, reconstruction_error, gradient = self.contrastive_divergence(sigma, beta, number_sampling_steps,\n",
    "                                                                           monitor_sampling = monitor_sampling,\n",
    "                                                                           calculate_loss = calculate_loss)\n",
    "        \n",
    "        noise = torch.randn((self.N, self.P), device = self.training_device, generator = self.random_number_generator)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gradient = alpha * gradient - decay_rate * self.xi\n",
    "            \n",
    "            self.descent_vector.copy_(momentum * self.descent_vector + learning_rate * gradient + torch.sqrt(torch.tensor(2 * (1 - momentum) / self.N)) * noise)\n",
    "            \n",
    "            self.xi.copy_(self.xi + learning_rate * self.descent_vector)\n",
    "            \n",
    "            # if normalize:\n",
    "                # self.xi.copy_(self.xi / torch.linalg.norm(self.xi, ord = 2, dim = 0))\n",
    "        \n",
    "        return loss, reconstruction_error\n",
    "    \n",
    "    # @torch.jit.export\n",
    "    def train_weights(self, loader : torch.utils.data.DataLoader, beta : float, alpha : float, initial_learning_rate : float,\n",
    "                      decay_rate : float, momentum : float, number_sampling_steps : int,\n",
    "                      number_training_epochs : int, number_monitored_training_epochs : int, monitor_sampling : bool):\n",
    "        \n",
    "        for training_epoch in range(1, number_training_epochs + 1):\n",
    "            if number_monitored_training_epochs != 0:\n",
    "                monitor_training_this_epoch = training_epoch % (number_training_epochs // number_monitored_training_epochs) == 0\n",
    "            else:\n",
    "                monitor_training_this_epoch = False\n",
    "            \n",
    "            # learning_rate = initial_learning_rate / (1 + training_epoch)\n",
    "            learning_rate = initial_learning_rate\n",
    "            \n",
    "            average_loss = torch.tensor(0.)\n",
    "            average_reconstruction_error = torch.tensor(0.)\n",
    "            \n",
    "            for batch, sigma_batch in enumerate(loader):\n",
    "                \n",
    "                if number_monitored_training_epochs != 0:\n",
    "                    monitor_sampling_this_epoch = monitor_training_this_epoch & monitor_sampling\n",
    "                else:\n",
    "                    monitor_sampling_this_epoch = False\n",
    "                \n",
    "                sigma_batch = sigma_batch.view(-1, self.N).to(self.training_device)\n",
    "                \n",
    "                loss, reconstruction_error = self.weights_update(sigma_batch, beta, alpha, learning_rate, decay_rate, momentum, number_sampling_steps,\n",
    "                                                                 monitor_sampling = monitor_sampling_this_epoch, calculate_loss = monitor_training_this_epoch)\n",
    "                \n",
    "                if monitor_training_this_epoch:\n",
    "                    average_loss += (loss.detach().item() - average_loss) / (batch + 1)\n",
    "                    average_reconstruction_error = (reconstruction_error.detach().item() - average_reconstruction_error) / (batch + 1)\n",
    "            \n",
    "            if monitor_training_this_epoch:\n",
    "                print(\"Epoch [{}/{}], loss: {:.4f}, reconstruction error: {:.4f}\"\n",
    "                      .format(training_epoch, number_training_epochs, average_loss, average_reconstruction_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM_combination():\n",
    "    def __init__(self, RBM_A, RBM_B, beta : float):\n",
    "        \n",
    "        self.xi = jnp.array([[RBM_A.xi.detach().cpu().numpy()],\n",
    "                             [RBM_B.xi.detach().cpu().numpy()]])\n",
    "        ### (2, 1, N, P)\n",
    "        \n",
    "        # self.xi = jnp.array([RBM_A.xi.detach().cpu().numpy(), RBM_B.xi.detach().cpu().numpy()])\n",
    "        # (2, N, P)\n",
    "        \n",
    "        self.beta = beta\n",
    "    \n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def energy(self, sigma):\n",
    "        \n",
    "        H = self.beta * sigma @ self.xi\n",
    "        ### (2, 2, M, P) = (2, M, N) @ (2, 1, N, P)\n",
    "        \n",
    "        # H = self.beta * sigma @ self.xi\n",
    "        # (2, M, P) = (M, N) @ (2, N, P)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    # def free_entropy(self, H_A, H_B, t : float):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def free_entropy(self, H, t : float):\n",
    "        \n",
    "        # f = jax.sum(logcosh((1 - t) * H_A), axis = -1) + jax.sum(logcosh(t * H_B), axis = -1)\n",
    "        f = jnp.sum(jnp.sum(jax_logcosh(t * H), axis = -1), axis = 0)\n",
    "        ### (2, M)\n",
    "        \n",
    "        # f = jnp.sum(jnp.sum(jax_logcosh(t * H), axis = -1), axis = 0)\n",
    "        # (M) = reduce((2, M, P), axes = (0, 2))\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    # def free_entropy_A(self, sigma, beta : float):\n",
    "        # f = jnp.sum(logcosh(beta * sigma @ self.xi_A), axis = -1)\n",
    "        \n",
    "        # return f\n",
    "    \n",
    "    # def free_entropy_B(self, sigma, beta : float):\n",
    "        # f = jnp.sum(logcosh(beta * sigma @ self.xi_B), axis = -1)\n",
    "        \n",
    "        # return f\n",
    "    \n",
    "    # def sample_hidden_given_visible(self, H_A, H_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def sample_hidden_given_visible(self, H, t : float, key : int):\n",
    "        # key, key_tau_A, key_tau_B = jax.random.split(key, num = 3)\n",
    "        key, key_tau = jax.random.split(key, num = 2)\n",
    "        \n",
    "        # P_tau_A_given_sigma = jax.nn.sigmoid((1 - t) * H_A)\n",
    "        # tau_A = jax_rademacher(P_tau_A_given_sigma, key_tau_A)\n",
    "        \n",
    "        # P_tau_B_given_sigma = jax.nn.sigmoid(t * H_B)\n",
    "        # tau_B = jax_rademacher(P_tau_B_given_sigma, key_tau_B)\n",
    "        \n",
    "        P_tau_given_sigma = jax.nn.sigmoid(t * H)\n",
    "        tau = jax_rademacher(P_tau_given_sigma, key_tau)\n",
    "        ### (2, 2, M, P)\n",
    "        \n",
    "        # P_tau_given_sigma = jax.nn.sigmoid(t * H)\n",
    "        # tau = jax_rademacher(P_tau_given_sigma, key_tau)\n",
    "        # (2, M, P)\n",
    "        \n",
    "        # return tau_A, tau_B, key\n",
    "        return tau, key\n",
    "    \n",
    "    # def sample_visible_given_hidden(self, tau_A, tau_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def sample_visible_given_hidden(self, tau, t : float, key : int):\n",
    "        key, key_sigma = jax.random.split(key, num = 2)\n",
    "        \n",
    "        # P_sigma_given_tau = jax.nn.sigmoid((1 - t) * self.beta * tau_A @ torch.transpose(self.xi_A, (0, 1)) + t * self.beta * tau_B @ torch.transpose(self.xi_B, (0, 1)))\n",
    "        # sigma = jax_rademacher(P_sigma_given_tau, key_sigma)\n",
    "        \n",
    "        P_sigma_given_tau = jax.nn.sigmoid(jnp.sum(t * self.beta * tau @ jnp.transpose(self.xi, (0, 1, 3, 2)), axis = 0))\n",
    "        sigma = jax_rademacher(P_sigma_given_tau, key_sigma)\n",
    "        \n",
    "        # P_sigma_given_tau = jax.nn.sigmoid(jnp.sum(t * self.beta * tau @ jnp.transpose(self.xi, (0, 2, 1)), axis = 0))\n",
    "        # sigma = jax_rademacher(P_sigma_given_tau, key_sigma)\n",
    "        # (2, M, P) = (M, N) @ (2, N, P)\n",
    "        # (M, N) = sum((2, M, P) @ (2, P, N), axis = 0)\n",
    "        \n",
    "        return sigma, key\n",
    "    \n",
    "    # def update_visible(self, H_A, H_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def update_visible(self, H, t : float, key : int):\n",
    "        \n",
    "        tau, key = self.sample_hidden_given_visible(H, t, key)\n",
    "        \n",
    "        sigma, key = self.sample_visible_given_hidden(tau, t, key)\n",
    "        \n",
    "        return sigma, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM_combination():\n",
    "    def __init__(self, RBM_A, RBM_B, beta : float):\n",
    "        \n",
    "        ### self.xi = jnp.array([[RBM_A.xi.detach().cpu().numpy()],\n",
    "                             ### [RBM_B.xi.detach().cpu().numpy()]])\n",
    "        ### (2, 1, N, P)\n",
    "        \n",
    "        self.xi = jnp.array([RBM_A.xi.detach().cpu().numpy(), RBM_B.xi.detach().cpu().numpy()])\n",
    "        # (2, N, P)\n",
    "        \n",
    "        self.beta = beta\n",
    "    \n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def energy(self, sigma):\n",
    "        \n",
    "        ### H = self.beta * sigma @ self.xi\n",
    "        ### (2, 2, M, P) = (2, M, N) @ (2, 1, N, P)\n",
    "        \n",
    "        H = self.beta * sigma @ self.xi\n",
    "        # (2, M, P) = (M, N) @ (2, N, P)\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    # def free_entropy(self, H_A, H_B, t : float):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def free_entropy(self, H, t : float):\n",
    "        \n",
    "        # f = jax.sum(logcosh((1 - t) * H_A), axis = -1) + jax.sum(logcosh(t * H_B), axis = -1)\n",
    "        ### f = jnp.sum(jnp.sum(jax_logcosh(t * H), axis = -1), axis = 0)\n",
    "        ### (2, M)\n",
    "        \n",
    "        f = jnp.sum(jax_logcosh(t * H), axis = -1)\n",
    "        \n",
    "        # student.free_entropy = torch.sum(logcosh(beta * sigma @ self.xi), dim = 1)\n",
    "        # torch.mean(student_A.free_entropy(sigma_teacher, beta)) - torch.mean(student_B.free_entropy(sigma_teacher, beta)) + f_difference\n",
    "        # (2, M) = reduce((2, M, P), axes = (2))\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    # def free_entropy_A(self, sigma, beta : float):\n",
    "        # f = jnp.sum(logcosh(beta * sigma @ self.xi_A), axis = -1)\n",
    "        \n",
    "        # return f\n",
    "    \n",
    "    # def free_entropy_B(self, sigma, beta : float):\n",
    "        # f = jnp.sum(logcosh(beta * sigma @ self.xi_B), axis = -1)\n",
    "        \n",
    "        # return f\n",
    "    \n",
    "    # def sample_hidden_given_visible(self, H_A, H_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def sample_hidden_given_energy(self, H, t : float, key : int):\n",
    "        # key, key_tau_A, key_tau_B = jax.random.split(key, num = 3)\n",
    "        key, key_tau = jax.random.split(key, num = 2)\n",
    "        \n",
    "        # P_tau_A_given_sigma = jax.nn.sigmoid((1 - t) * H_A)\n",
    "        # tau_A = jax_rademacher(P_tau_A_given_sigma, key_tau_A)\n",
    "        \n",
    "        # P_tau_B_given_sigma = jax.nn.sigmoid(t * H_B)\n",
    "        # tau_B = jax_rademacher(P_tau_B_given_sigma, key_tau_B)\n",
    "        \n",
    "        ### P_tau_given_sigma = jax.nn.sigmoid(t * H)\n",
    "        ### tau = jax_rademacher(P_tau_given_sigma, key_tau)\n",
    "        ### (2, 2, M, P)\n",
    "        \n",
    "        P_tau_given_sigma = jax.nn.sigmoid(t * H)\n",
    "        tau = jax_rademacher(P_tau_given_sigma, key_tau)\n",
    "        # (2, M, P)\n",
    "        \n",
    "        # return tau_A, tau_B, key\n",
    "        return tau, key\n",
    "    \n",
    "    # def sample_visible_given_hidden(self, tau_A, tau_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def sample_energy_given_hidden(self, tau, t : float, key : int):\n",
    "        key, key_sigma = jax.random.split(key, num = 2)\n",
    "        \n",
    "        # P_sigma_given_tau = jax.nn.sigmoid((1 - t) * self.beta * tau_A @ torch.transpose(self.xi_A, (0, 1)) + t * self.beta * tau_B @ torch.transpose(self.xi_B, (0, 1)))\n",
    "        # sigma = jax_rademacher(P_sigma_given_tau, key_sigma)\n",
    "        \n",
    "        ### P_sigma_given_tau = jax.nn.sigmoid(jnp.sum(t * self.beta * tau @ jnp.transpose(self.xi, (0, 1, 3, 2)), axis = 0))\n",
    "        ### sigma = jax_rademacher(P_sigma_given_tau, key_sigma)\n",
    "        \n",
    "        P_sigma_given_tau = jax.nn.sigmoid(jnp.sum(t * self.beta * tau @ jnp.transpose(self.xi, (0, 2, 1)), axis = 0))\n",
    "        sigma = jax_rademacher(P_sigma_given_tau, key_sigma)\n",
    "        # (2, M, P) = (M, N) @ (2, N, P)\n",
    "        # (M, N) = sum((2, M, P) @ (2, P, N), axis = 0)\n",
    "        H = self.energy(sigma)\n",
    "        \n",
    "        return H, key\n",
    "    \n",
    "    # def update_visible(self, H_A, H_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def update_energy(self, H, t : float, key : int):\n",
    "        \n",
    "        tau, key = self.sample_hidden_given_visible(H, t, key)\n",
    "        \n",
    "        H, key = self.sample_visible_given_hidden(tau, t, key)\n",
    "        \n",
    "        return H, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM_combination():\n",
    "    def __init__(self, RBM_A, RBM_B, beta : float):\n",
    "        \n",
    "        self.xi_A = RBM_A.xi.detach().cpu().numpy()\n",
    "        self.xi_B = RBM_B.xi.detach().cpu().numpy()\n",
    "        # (N, P)\n",
    "        \n",
    "        self.beta = beta\n",
    "    \n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def energy(self, sigma):\n",
    "        \n",
    "        ### (M, P) = (M, N) @ (N, P)\n",
    "        \n",
    "        # H_A = self.beta * sigma @ self.xi_A\n",
    "        # H_B = self.beta * sigma @ self.xi_B\n",
    "        \n",
    "        H_A, H_B = self.beta * sigma @ jnp.array([self.xi_A, self.xi_B])\n",
    "        \n",
    "        return H_A, H_B\n",
    "    \n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def free_entropy(self, H):\n",
    "        f = jnp.sum(jax_logcosh(H), axis = -1)\n",
    "        \n",
    "        return f\n",
    "    \n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def sample_hidden_given_energy(self, H_A, H_B, t : float, key : int):\n",
    "        # key, key_tau_A, key_tau_B = jax.random.split(key, num = 3)\n",
    "        key, key_tau = jax.random.split(key, num = 2)\n",
    "        \n",
    "        # P_tau_A_given_sigma = jax.nn.sigmoid((1 - t) * H_A)\n",
    "        # tau_A = jax_rademacher(P_tau_A_given_sigma, key_tau_A)\n",
    "        \n",
    "        # P_tau_B_given_sigma = jax.nn.sigmoid(t * H_B)\n",
    "        # tau_B = jax_rademacher(P_tau_B_given_sigma, key_tau_B)\n",
    "        \n",
    "        P_tau_given_sigma = jax.nn.sigmoid(jnp.array([(1 - t) * H_A, t * H_B]))\n",
    "        tau_A, tau_B = jax_rademacher(P_tau_given_sigma, key_tau)\n",
    "        \n",
    "        return tau_A, tau_B, key\n",
    "    \n",
    "    # def sample_visible_given_hidden(self, tau_A, tau_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def sample_energy_given_hidden(self, tau_A, tau_B, t : float, key : int):\n",
    "        key, key_sigma = jax.random.split(key, num = 2)\n",
    "        \n",
    "        # tau_A = (1 - t) * self.beta * tau_A\n",
    "        # tau_B = t * self.beta * tau_B\n",
    "        \n",
    "        P_sigma_given_tau = jax.nn.sigmoid((1 - t) * self.beta * tau_A @ jnp.transpose(self.xi_A, (1, 0))\n",
    "                                           + t * self.beta * tau_B @ jnp.transpose(self.xi_B, (1, 0)))\n",
    "        \n",
    "        sigma = jax_rademacher(P_sigma_given_tau, key_sigma)\n",
    "        \n",
    "        H_A, H_B = self.energy(sigma)\n",
    "        \n",
    "        return H_A, H_B, key\n",
    "    \n",
    "    # def update_visible(self, H_A, H_B, t : float, key : int):\n",
    "    @partial(jax.jit, static_argnums = 0)\n",
    "    def update_energy(self, H_A, H_B, t : float, key : int):\n",
    "        \n",
    "        tau_A, tau_B, key = self.sample_hidden_given_energy(H_A, H_B, t, key)\n",
    "        \n",
    "        H_A, H_B, key = self.sample_energy_given_hidden(tau_A, tau_B, t, key)\n",
    "        \n",
    "        return H_A, H_B, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_annealing_steps + 1, partial(student_combination, beta, number_annealing_steps),\n",
    "                                                            # (f_difference, H_A, H_B, student_combination.key)\n",
    "@partial(jax.jit, static_argnums = (0, 1))\n",
    "# @partial(jax.jit, static_argnums = (0, 1, 2))\n",
    "def free_entropy_difference(student_combination, number_annealing_steps, annealing_step, carry_over):\n",
    "# def free_entropy_difference(student_combination, number_annealing_steps, iterate_backward, annealing_step, carry_over):\n",
    "    f_difference, H, key = carry_over\n",
    "    \n",
    "    t = jax.numpy.array([[1 - annealing_step/number_annealing_steps, annealing_step/number_annealing_steps],\n",
    "                         [annealing_step/number_annealing_steps, 1 - annealing_step/number_annealing_steps]])[..., jnp.newaxis, jnp.newaxis]\n",
    "    \n",
    "    # if iterate_backward:\n",
    "        # t = jax.numpy.array([annealing_step/number_annealing_steps, 1 - annealing_step/number_annealing_steps])[..., jnp.newaxis, jnp.newaxis]\n",
    "    # else:\n",
    "        # t = jax.numpy.array([1 - annealing_step/number_annealing_steps, annealing_step/number_annealing_steps])[..., jnp.newaxis, jnp.newaxis]\n",
    "    \n",
    "    f_difference += student_combination.free_entropy(H, t)\n",
    "    \n",
    "    sigma, key = student_combination.update_visible(H, t, key)\n",
    "    \n",
    "    H = student_combination.energy(sigma)\n",
    "    \n",
    "    f_difference -= student_combination.free_entropy(H, t)\n",
    "    \n",
    "    return f_difference, H, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_annealing_steps + 1, partial(student_combination, beta, number_annealing_steps),\n",
    "                                                            # (f_difference, H_A, H_B, student_combination.key)\n",
    "### @partial(jax.jit, static_argnums = (0, 1))\n",
    "@partial(jax.jit, static_argnums = (0, 1, 2))\n",
    "### def free_entropy_difference(student_combination, number_annealing_steps, annealing_step, carry_over):\n",
    "def free_entropy_difference(student_combination, number_annealing_steps, iterate_backward, annealing_step, carry_over):\n",
    "    f_difference, H, key = carry_over\n",
    "    \n",
    "    # t = jax.numpy.array([[1 - annealing_step/number_annealing_steps, annealing_step/number_annealing_steps],\n",
    "                         # [annealing_step/number_annealing_steps, 1 - annealing_step/number_annealing_steps]])[..., jnp.newaxis, jnp.newaxis]\n",
    "    \n",
    "    if iterate_backward:\n",
    "        t = jax.numpy.array([annealing_step/number_annealing_steps, 1 - annealing_step/number_annealing_steps])[..., jnp.newaxis, jnp.newaxis]\n",
    "    else:\n",
    "        t = jax.numpy.array([1 - annealing_step/number_annealing_steps, annealing_step/number_annealing_steps])[..., jnp.newaxis, jnp.newaxis]\n",
    "    \n",
    "    f_difference += jnp.sum(student_combination.free_entropy(H, t), axis = 0)\n",
    "    \n",
    "    H, key = student_combination.update_energy(H, t, key)\n",
    "    \n",
    "    f_difference -= jnp.sum(student_combination.free_entropy(H, t), axis = 0)\n",
    "    \n",
    "    return f_difference, H, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums = (0, 1, 2))\n",
    "def free_entropy_difference(student_combination, number_annealing_steps, iterate_backward, annealing_step, carry_over):\n",
    "    f_difference, H_A, H_B, key = carry_over\n",
    "    \n",
    "    # t = jax.numpy.array([[1 - annealing_step/number_annealing_steps, annealing_step/number_annealing_steps],\n",
    "                         # [annealing_step/number_annealing_steps, 1 - annealing_step/number_annealing_steps]])[..., jnp.newaxis, jnp.newaxis]\n",
    "    \n",
    "    if iterate_backward:\n",
    "        t = 1 - annealing_step/number_annealing_steps\n",
    "    else:\n",
    "        t = annealing_step/number_annealing_steps\n",
    "    \n",
    "    f_difference += (1 - t) * student_combination.free_entropy(H_A) + t * student_combination.free_entropy(H_B)\n",
    "    \n",
    "    H_A, H_B, key = student_combination.update_energy(H_A, H_B, t, key)\n",
    "    \n",
    "    f_difference += -(1 - t) * student_combination.free_entropy(H_A) - t * student_combination.free_entropy(H_B)\n",
    "    \n",
    "    return f_difference, H_A, H_B, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(teacher, student_A, student_B, beta, M, number_sampling_steps,\n",
    "             number_annealing_steps, number_annealing_runs,\n",
    "             number_monitored_sampling_steps, number_monitored_annealing_runs, seed):\n",
    "    \n",
    "    student_combination = RBM_combination(student_A, student_B, beta)\n",
    "    \n",
    "    device = teacher.training_device\n",
    "    \n",
    "    N = teacher.N\n",
    "    # P = teacher.P\n",
    "    # P_1 = student_1.P\n",
    "    # P_2 = student_2.P\n",
    "    \n",
    "    alpha = M/N\n",
    "    \n",
    "    random_number_generator = teacher.random_number_generator\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    sigma_teacher_torch = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "    sigma_student_torch = torch.nn.Parameter(torch.zeros((2, M, N), device = device), requires_grad = False)\n",
    "    \n",
    "    # number_sampling_steps = 10\n",
    "    # number_monitored_sampling_steps = 10\n",
    "    \n",
    "    # number_annealing_runs = 20\n",
    "    \n",
    "    t_i = jnp.array([[1., 0.],\n",
    "                     [0., 1.]])[..., jnp.newaxis, jnp.newaxis]\n",
    "            \n",
    "    t_f = jnp.array([[0., 1.],\n",
    "                     [1., 0.]])[..., jnp.newaxis, jnp.newaxis]\n",
    "            \n",
    "    t_diff = jnp.array([[1.],\n",
    "                        [-1.]])[..., jnp.newaxis, jnp.newaxis]\n",
    "    \n",
    "    mean_f_difference = 0\n",
    "    var_f_difference = 0\n",
    "        \n",
    "    mean_log_likelihood_difference = 0\n",
    "    var_log_likelihood_difference = 0\n",
    "    \n",
    "    for annealing_run in range(1, number_annealing_runs + 1):\n",
    "        \n",
    "        if number_monitored_annealing_runs != 0:\n",
    "            monitor_annealing_this_run = annealing_run % (number_annealing_runs // number_monitored_annealing_runs) == 0\n",
    "        else:\n",
    "            monitor_annealing_this_run = False\n",
    "        \n",
    "        sigma_teacher_torch.copy_(torch.sign(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator))))\n",
    "        sigma_student_torch.copy_(torch.sign(torch.sign(torch.randn((2, M, N), device = device, generator = random_number_generator))))\n",
    "        \n",
    "        teacher.sample_visible(sigma_teacher_torch, beta, number_sampling_steps,\n",
    "                               number_monitored_sampling_steps)\n",
    "        \n",
    "        student_A.sample_visible(sigma_student_torch[0], beta, number_sampling_steps,\n",
    "                                 number_monitored_sampling_steps)\n",
    "        \n",
    "        student_B.sample_visible(sigma_student_torch[1], beta, number_sampling_steps,\n",
    "                                 number_monitored_sampling_steps)\n",
    "        \n",
    "        sigma_teacher = jnp.array(sigma_teacher_torch.detach().cpu().numpy())\n",
    "        sigma_student = jnp.array(sigma_student_torch.detach().cpu().numpy())\n",
    "        \n",
    "        H = student_combination.energy(sigma_student)\n",
    "        \n",
    "        sigma_student, key = student_combination.update_visible(H, t_i, key)\n",
    "        \n",
    "        H = student_combination.energy(sigma_student)\n",
    "        \n",
    "        f_difference = -student_combination.free_entropy(H, t_i)\n",
    "        \n",
    "        ### Bottleneck!\n",
    "        start = timer()\n",
    "        \n",
    "        f_difference, H, key = jax.lax.fori_loop(1, number_annealing_steps + 1, partial(free_entropy_difference, student_combination, number_annealing_steps),\n",
    "                                                 (f_difference, H, key))\n",
    "        \n",
    "        end = timer()\n",
    "        print(\"Time elapsed: %.4f.\" % (end - start))\n",
    "        \n",
    "        # f_difference.copy_(f_difference + student_B.free_entropy(sigma_student, beta))\n",
    "        f_difference += student_combination.free_entropy(H, t_f)\n",
    "        \n",
    "        c = jnp.max(f_difference, axis = -1, keepdims = True)\n",
    "        f_difference = jnp.squeeze(c) + jnp.log(jnp.mean(jnp.exp(f_difference - c), axis = -1))\n",
    "        f_difference = f_difference.at[1].multiply(-1.)\n",
    "        \n",
    "        H = student_combination.energy(sigma_teacher)\n",
    "        \n",
    "        # jnp.mean(student_combination.free_entropy(H, t_diff))\n",
    "        \n",
    "        log_likelihood_difference = jnp.mean(student_combination.free_entropy(H, t_diff)) + f_difference\n",
    "        \n",
    "        # log_posterior_difference = log_likelihood_difference\n",
    "        \n",
    "        if monitor_annealing_this_run:\n",
    "            print(\"Run [{}/{}], free entropy difference: {:.4f}, {:.4f}\".format(annealing_run, number_annealing_runs, np.mean(f_difference[0]), np.mean(f_difference[1])))\n",
    "        \n",
    "        mean_f_difference += (f_difference - mean_f_difference) / annealing_run # (annealing_run + 1)\n",
    "        var_f_difference += (f_difference - mean_f_difference)**2 / annealing_run # (annealing_run + 1)\n",
    "        var_f_difference *= (annealing_run - 1) / annealing_run # annealing_run / (annealing_run + 1)\n",
    "        \n",
    "        mean_log_likelihood_difference += (log_likelihood_difference - mean_log_likelihood_difference) / annealing_run # (annealing_run + 1)\n",
    "        var_log_likelihood_difference += (log_likelihood_difference - mean_log_likelihood_difference)**2 / annealing_run # (annealing_run + 1)\n",
    "        var_log_likelihood_difference *= (annealing_run - 1) / annealing_run # annealing_run / (annealing_run + 1)\n",
    "    \n",
    "    del sigma_teacher_torch\n",
    "    del sigma_student_torch\n",
    "    gc.collect()\n",
    "    \n",
    "    var_f_difference *= number_annealing_runs / (number_annealing_runs - 1)\n",
    "    var_log_likelihood_difference *= number_annealing_runs / (number_annealing_runs - 1)\n",
    "    \n",
    "    # mean_log_posterior_difference = mean_log_likelihood_difference + 1/alpha * 1/2 * torch.sum(student_B.xi**2) - 1/alpha * 1/2 * torch.sum(student_A.xi**2)\n",
    "    mean_log_posterior_difference = mean_log_likelihood_difference - 1/alpha * 1/2 * np.squeeze(t_diff * jnp.sum(student_combination.xi**2,\n",
    "                                                                                                                 axis = (-2, -1), keepdims = True))\n",
    "    var_log_posterior_difference = var_log_likelihood_difference\n",
    "    \n",
    "    return mean_f_difference, np.sqrt(var_f_difference), mean_log_likelihood_difference, np.sqrt(var_log_likelihood_difference), mean_log_posterior_difference, np.sqrt(var_log_posterior_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(teacher, student_A, student_B, beta, M, number_sampling_steps,\n",
    "             number_annealing_steps, number_annealing_runs,\n",
    "             number_monitored_sampling_steps, number_monitored_annealing_runs, seed):\n",
    "    \n",
    "    student_combination = RBM_combination(student_A, student_B, beta)\n",
    "    \n",
    "    device = teacher.training_device\n",
    "    \n",
    "    N = teacher.N\n",
    "    # P = teacher.P\n",
    "    # P_1 = student_1.P\n",
    "    # P_2 = student_2.P\n",
    "    \n",
    "    alpha = M/N\n",
    "    \n",
    "    random_number_generator = teacher.random_number_generator\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    sigma_teacher_torch = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "    # sigma_student_torch = torch.nn.Parameter(torch.zeros((2, M, N), device = device), requires_grad = False)\n",
    "    sigma_student_torch = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "    \n",
    "    # number_sampling_steps = 10\n",
    "    # number_monitored_sampling_steps = 10\n",
    "    \n",
    "    # number_annealing_runs = 20\n",
    "            \n",
    "    t_diff = jnp.array([1., 1.])[..., jnp.newaxis, jnp.newaxis]\n",
    "    \n",
    "    f_difference = jnp.array([0., 0.])\n",
    "    \n",
    "    mean_f_difference = jnp.array([0., 0.])\n",
    "    var_f_difference = jnp.array([0., 0.])\n",
    "        \n",
    "    mean_log_likelihood_difference = jnp.array([0., 0.])\n",
    "    var_log_likelihood_difference = jnp.array([0., 0.])\n",
    "    \n",
    "    # iterate_backward = [False, True]\n",
    "    students = [student_A, student_B]\n",
    "    \n",
    "    for annealing_run in range(1, number_annealing_runs + 1):\n",
    "        \n",
    "        if number_monitored_annealing_runs != 0:\n",
    "            monitor_annealing_this_run = annealing_run % (number_annealing_runs // number_monitored_annealing_runs) == 0\n",
    "        else:\n",
    "            monitor_annealing_this_run = False\n",
    "        \n",
    "        sigma_teacher_torch.copy_(torch.sign(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator))))\n",
    "        \n",
    "        teacher.sample_visible(sigma_teacher_torch, beta, number_sampling_steps,\n",
    "                               number_monitored_sampling_steps)\n",
    "        \n",
    "        sigma_teacher = jnp.array(sigma_teacher_torch.detach().cpu().numpy())\n",
    "        \n",
    "        for j, student in enumerate(students):\n",
    "            iterate_backward = (j == 1)\n",
    "            \n",
    "            if iterate_backward:\n",
    "                t_ini = jnp.array([0., 1.])[..., jnp.newaxis, jnp.newaxis]\n",
    "                t_fin = jnp.array([1., 0.])[..., jnp.newaxis, jnp.newaxis]\n",
    "            else:\n",
    "                t_ini = jnp.array([1., 0.])[..., jnp.newaxis, jnp.newaxis]\n",
    "                t_fin = jnp.array([0., 1.])[..., jnp.newaxis, jnp.newaxis]\n",
    "            \n",
    "            # sigma_student_torch.copy_(torch.sign(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator))))\n",
    "            sigma_student_torch.copy_(sigma_teacher_torch)\n",
    "            \n",
    "            student.sample_visible(sigma_student_torch, beta, number_sampling_steps,\n",
    "                                   number_monitored_sampling_steps)\n",
    "            \n",
    "            sigma_student = jnp.array(sigma_student_torch.detach().cpu().numpy())\n",
    "            \n",
    "            H = student_combination.energy(sigma_student)\n",
    "            \n",
    "            # sigma_student, key = student_combination.update_visible(H, t_ini, key)\n",
    "            \n",
    "            # H = student_combination.energy(sigma_student)\n",
    "            \n",
    "            f_difference_cur = -jnp.sum(student_combination.free_entropy(H, t_ini), axis = 0)\n",
    "            \n",
    "            start = timer()\n",
    "            \n",
    "            ### Bottleneck!\n",
    "            f_difference_cur, H, key = jax.lax.fori_loop(1, number_annealing_steps + 1,\n",
    "                                                         partial(free_entropy_difference, student_combination, number_annealing_steps, iterate_backward),\n",
    "                                                         (f_difference_cur, H, key))\n",
    "            \n",
    "            end = timer()\n",
    "            print(\"Time elapsed: %.4f.\" % (end - start))\n",
    "            \n",
    "            f_difference_cur += jnp.sum(student_combination.free_entropy(H, t_fin), axis = 0)\n",
    "            \n",
    "            c = jnp.max(f_difference_cur, axis = -1)\n",
    "            f_difference = f_difference.at[j].set(c + jnp.log(jnp.mean(jnp.exp(f_difference_cur - c), axis = -1)))\n",
    "        \n",
    "        f_difference = f_difference.at[1].multiply(-1.)\n",
    "        \n",
    "        H = student_combination.energy(sigma_teacher)\n",
    "        \n",
    "        # jnp.mean(student_combination.free_entropy(H, t_diff))\n",
    "        \n",
    "        f_numerator = student_combination.free_entropy(H, t_diff)\n",
    "        f_numerator = f_numerator[0] - f_numerator[1]\n",
    "        \n",
    "        log_likelihood_difference = jnp.mean(f_numerator) + f_difference\n",
    "        \n",
    "        # log_posterior_difference = log_likelihood_difference\n",
    "        \n",
    "        if monitor_annealing_this_run:\n",
    "            print(\"Run [{}/{}], free entropy difference: {:.4f}, {:.4f}\".format(annealing_run, number_annealing_runs, f_difference[0], f_difference[1]))\n",
    "        \n",
    "        mean_f_difference += (f_difference - mean_f_difference) / annealing_run # (annealing_run + 1)\n",
    "        var_f_difference += (f_difference - mean_f_difference)**2 / annealing_run # (annealing_run + 1)\n",
    "        var_f_difference *= (annealing_run - 1) / annealing_run # annealing_run / (annealing_run + 1)\n",
    "        \n",
    "        mean_log_likelihood_difference += (log_likelihood_difference - mean_log_likelihood_difference) / annealing_run # (annealing_run + 1)\n",
    "        var_log_likelihood_difference += (log_likelihood_difference - mean_log_likelihood_difference)**2 / annealing_run # (annealing_run + 1)\n",
    "        var_log_likelihood_difference *= (annealing_run - 1) / annealing_run # annealing_run / (annealing_run + 1)\n",
    "    \n",
    "    del sigma_teacher_torch\n",
    "    del sigma_student_torch\n",
    "    gc.collect()\n",
    "    \n",
    "    var_f_difference *= number_annealing_runs / (number_annealing_runs - 1)\n",
    "    var_log_likelihood_difference *= number_annealing_runs / (number_annealing_runs - 1)\n",
    "    \n",
    "    # mean_log_posterior_difference = mean_log_likelihood_difference + 1/alpha * 1/2 * torch.sum(student_B.xi**2) - 1/alpha * 1/2 * torch.sum(student_A.xi**2)\n",
    "    mean_log_posterior_difference = mean_log_likelihood_difference - 1/alpha * 1/2 * jnp.sum(t_diff * jnp.sum(student_combination.xi**2,\n",
    "                                                                                                              axis = (-2, -1), keepdims = True))\n",
    "    var_log_posterior_difference = var_log_likelihood_difference\n",
    "    \n",
    "    return mean_f_difference, np.sqrt(var_f_difference), mean_log_likelihood_difference, np.sqrt(var_log_likelihood_difference), mean_log_posterior_difference, np.sqrt(var_log_posterior_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(teacher, student_A, student_B, beta, M, number_sampling_steps,\n",
    "             number_annealing_steps, number_annealing_runs,\n",
    "             number_monitored_sampling_steps, number_monitored_annealing_runs, seed):\n",
    "    \n",
    "    student_combination = RBM_combination(student_A, student_B, beta)\n",
    "    \n",
    "    device = teacher.training_device\n",
    "    \n",
    "    N = teacher.N\n",
    "    # P = teacher.P\n",
    "    # P_1 = student_1.P\n",
    "    # P_2 = student_2.P\n",
    "    \n",
    "    alpha = M/N\n",
    "    \n",
    "    random_number_generator = teacher.random_number_generator\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    \n",
    "    sigma_teacher_torch = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "    # sigma_student_torch = torch.nn.Parameter(torch.zeros((2, M, N), device = device), requires_grad = False)\n",
    "    sigma_student_torch = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "    \n",
    "    # number_sampling_steps = 10\n",
    "    # number_monitored_sampling_steps = 10\n",
    "    \n",
    "    # number_annealing_runs = 20\n",
    "    \n",
    "    mean_f_difference = jnp.array([0., 0.])\n",
    "    var_f_difference = jnp.array([0., 0.])\n",
    "        \n",
    "    mean_log_likelihood_difference = jnp.array([0., 0.])\n",
    "    var_log_likelihood_difference = jnp.array([0., 0.])\n",
    "    \n",
    "    # iterate_backward = [False, True]\n",
    "    students = [student_A, student_B]\n",
    "    \n",
    "    for annealing_run in range(1, number_annealing_runs + 1):\n",
    "        \n",
    "        if number_monitored_annealing_runs != 0:\n",
    "            monitor_annealing_this_run = annealing_run % (number_annealing_runs // number_monitored_annealing_runs) == 0\n",
    "        else:\n",
    "            monitor_annealing_this_run = False\n",
    "        \n",
    "        sigma_teacher_torch.copy_(torch.sign(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator))))\n",
    "        \n",
    "        teacher.sample_visible(sigma_teacher_torch, beta, number_sampling_steps,\n",
    "                               number_monitored_sampling_steps)\n",
    "        \n",
    "        sigma_teacher = jnp.array(sigma_teacher_torch.detach().cpu().numpy())\n",
    "        \n",
    "        f_difference = jnp.array([0., 0.])\n",
    "        f_difference_cur = jnp.zeros(M)\n",
    "        \n",
    "        for j, student in enumerate(students):\n",
    "            \n",
    "            iterate_backward = (j == 1)\n",
    "            t_ini = np.float32(j)\n",
    "            t_fin = 1. - t_ini\n",
    "            \n",
    "            sigma_student_torch.copy_(torch.sign(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator))))\n",
    "            # sigma_student_torch.copy_(sigma_teacher_torch)\n",
    "            \n",
    "            student.sample_visible(sigma_student_torch, beta, number_sampling_steps,\n",
    "                                   number_monitored_sampling_steps)\n",
    "            \n",
    "            sigma_student = jnp.array(sigma_student_torch.detach().cpu().numpy())\n",
    "            \n",
    "            H_A, H_B = student_combination.energy(sigma_student)\n",
    "            \n",
    "            # sigma_student, key = student_combination.update_visible(H, t_ini, key)\n",
    "            \n",
    "            # H = student_combination.energy(sigma_student)\n",
    "            \n",
    "            f_difference_cur += -(1 - t_ini) * student_combination.free_entropy(H_A) - t_ini * student_combination.free_entropy(H_B)\n",
    "            \n",
    "            start = timer()\n",
    "            \n",
    "            ### Bottleneck!\n",
    "            f_difference_cur, H_A, H_B, key = jax.lax.fori_loop(1, number_annealing_steps,\n",
    "                                                                partial(free_entropy_difference, student_combination, number_annealing_steps, iterate_backward),\n",
    "                                                                (f_difference_cur, H_A, H_B, key))\n",
    "            \n",
    "            end = timer()\n",
    "            print(\"Time elapsed: %.4f.\" % (end - start))\n",
    "            \n",
    "            f_difference_cur += (1 - t_fin) * student_combination.free_entropy(H_A) + t_fin * student_combination.free_entropy(H_B)\n",
    "            \n",
    "            c = jnp.max(f_difference_cur, axis = -1)\n",
    "            f_difference = f_difference.at[j].set(c + jnp.log(jnp.mean(jnp.exp(f_difference_cur - c), axis = -1)))\n",
    "        \n",
    "        f_difference = f_difference.at[1].multiply(-1.)\n",
    "        \n",
    "        H_A, H_B = student_combination.energy(sigma_teacher)\n",
    "        \n",
    "        log_likelihood_difference = jnp.mean(student_combination.free_entropy(H_A)) - jnp.mean(student_combination.free_entropy(H_B)) + f_difference\n",
    "        \n",
    "        # log_posterior_difference = log_likelihood_difference\n",
    "        \n",
    "        if monitor_annealing_this_run:\n",
    "            print(\"Run [{}/{}], free entropy difference: {:.4f}, {:.4f}\".format(annealing_run, number_annealing_runs, f_difference[0], f_difference[1]))\n",
    "            f_difference = analytical_free_energy_difference(student_combination)\n",
    "            print(f_difference)\n",
    "        \n",
    "        mean_f_difference += (f_difference - mean_f_difference) / annealing_run # (annealing_run + 1)\n",
    "        var_f_difference += (f_difference - mean_f_difference)**2 / annealing_run # (annealing_run + 1)\n",
    "        var_f_difference *= (annealing_run - 1) / annealing_run # annealing_run / (annealing_run + 1)\n",
    "        \n",
    "        mean_log_likelihood_difference += (log_likelihood_difference - mean_log_likelihood_difference) / annealing_run # (annealing_run + 1)\n",
    "        var_log_likelihood_difference += (log_likelihood_difference - mean_log_likelihood_difference)**2 / annealing_run # (annealing_run + 1)\n",
    "        var_log_likelihood_difference *= (annealing_run - 1) / annealing_run # annealing_run / (annealing_run + 1)\n",
    "    \n",
    "    del sigma_teacher_torch\n",
    "    del sigma_student_torch\n",
    "    gc.collect()\n",
    "    \n",
    "    var_f_difference *= number_annealing_runs / (number_annealing_runs - 1)\n",
    "    var_log_likelihood_difference *= number_annealing_runs / (number_annealing_runs - 1)\n",
    "    \n",
    "    # mean_log_posterior_difference = mean_log_likelihood_difference + 1/alpha * 1/2 * torch.sum(student_B.xi**2) - 1/alpha * 1/2 * torch.sum(student_A.xi**2)\n",
    "    mean_log_posterior_difference = mean_log_likelihood_difference + 1/2 * 1/alpha * (jnp.sum(student_combination.xi_B**2) - jnp.sum(student_combination.xi_A**2))\n",
    "    var_log_posterior_difference = var_log_likelihood_difference\n",
    "    \n",
    "    return mean_f_difference, np.sqrt(var_f_difference), mean_log_likelihood_difference, np.sqrt(var_log_likelihood_difference), mean_log_posterior_difference, np.sqrt(var_log_posterior_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ 2 GB per sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [0/50], free entropy: 5.1559\n",
      "Step [25/50], free entropy: 14.7833\n",
      "Step [50/50], free entropy: 14.7538\n",
      "Epoch [100/3000], loss: 7.7587, reconstruction error: 9.9800\n",
      "Epoch [200/3000], loss: 4.6092, reconstruction error: 8.8268\n",
      "Epoch [300/3000], loss: 2.6840, reconstruction error: 7.8814\n",
      "Epoch [400/3000], loss: 1.1724, reconstruction error: 7.2050\n",
      "Epoch [500/3000], loss: 0.3511, reconstruction error: 6.5575\n",
      "Epoch [600/3000], loss: -0.1981, reconstruction error: 6.0733\n",
      "Epoch [700/3000], loss: -0.4034, reconstruction error: 5.8065\n",
      "Epoch [800/3000], loss: -0.5930, reconstruction error: 5.5223\n",
      "Epoch [900/3000], loss: -0.6000, reconstruction error: 5.2862\n",
      "Epoch [1000/3000], loss: -0.5228, reconstruction error: 5.1892\n",
      "Epoch [1100/3000], loss: -0.5493, reconstruction error: 5.1105\n",
      "Epoch [1200/3000], loss: -0.4820, reconstruction error: 4.8985\n",
      "Epoch [1300/3000], loss: -0.4318, reconstruction error: 4.9004\n",
      "Epoch [1400/3000], loss: -0.3423, reconstruction error: 4.9164\n",
      "Epoch [1500/3000], loss: -0.2913, reconstruction error: 4.7599\n",
      "Epoch [1600/3000], loss: -0.2139, reconstruction error: 4.8167\n",
      "Epoch [1700/3000], loss: -0.0974, reconstruction error: 4.7825\n",
      "Epoch [1800/3000], loss: -0.0942, reconstruction error: 4.7425\n",
      "Epoch [1900/3000], loss: -0.0680, reconstruction error: 4.7323\n",
      "Epoch [2000/3000], loss: 0.0413, reconstruction error: 4.6981\n",
      "Epoch [2100/3000], loss: 0.0079, reconstruction error: 4.7474\n",
      "Epoch [2200/3000], loss: 0.0617, reconstruction error: 4.7267\n",
      "Epoch [2300/3000], loss: 0.0924, reconstruction error: 4.7014\n",
      "Epoch [2400/3000], loss: -0.0098, reconstruction error: 4.6867\n",
      "Epoch [2500/3000], loss: 0.0430, reconstruction error: 4.6487\n",
      "Epoch [2600/3000], loss: 0.0731, reconstruction error: 4.6962\n",
      "Epoch [2700/3000], loss: 0.0318, reconstruction error: 4.6281\n",
      "Epoch [2800/3000], loss: 0.0304, reconstruction error: 4.6249\n",
      "Epoch [2900/3000], loss: 0.0796, reconstruction error: 4.6263\n",
      "Epoch [3000/3000], loss: 0.0958, reconstruction error: 4.6863\n",
      "Epoch [100/3000], loss: 7.4943, reconstruction error: 10.3422\n",
      "Epoch [200/3000], loss: 4.3584, reconstruction error: 8.7026\n",
      "Epoch [300/3000], loss: 2.3698, reconstruction error: 7.5145\n",
      "Epoch [400/3000], loss: 1.1304, reconstruction error: 6.6796\n",
      "Epoch [500/3000], loss: 0.5662, reconstruction error: 6.0497\n",
      "Epoch [600/3000], loss: 0.2214, reconstruction error: 5.7159\n",
      "Epoch [700/3000], loss: -0.0134, reconstruction error: 5.4847\n",
      "Epoch [800/3000], loss: -0.0294, reconstruction error: 5.2140\n",
      "Epoch [900/3000], loss: -0.0149, reconstruction error: 5.0652\n",
      "Epoch [1000/3000], loss: -0.0483, reconstruction error: 5.0305\n",
      "Epoch [1100/3000], loss: -0.0429, reconstruction error: 4.9209\n",
      "Epoch [1200/3000], loss: 0.0048, reconstruction error: 4.8510\n",
      "Epoch [1300/3000], loss: 0.1833, reconstruction error: 4.8493\n",
      "Epoch [1400/3000], loss: 0.0484, reconstruction error: 4.7448\n",
      "Epoch [1500/3000], loss: 0.2401, reconstruction error: 4.7698\n",
      "Epoch [1600/3000], loss: 0.0623, reconstruction error: 4.7467\n",
      "Epoch [1700/3000], loss: 0.1492, reconstruction error: 4.6464\n",
      "Epoch [1800/3000], loss: 0.2228, reconstruction error: 4.7305\n",
      "Epoch [1900/3000], loss: 0.1371, reconstruction error: 4.7108\n",
      "Epoch [2000/3000], loss: 0.0904, reconstruction error: 4.7066\n",
      "Epoch [2100/3000], loss: 0.3016, reconstruction error: 4.6612\n",
      "Epoch [2200/3000], loss: 0.1969, reconstruction error: 4.6605\n",
      "Epoch [2300/3000], loss: 0.1552, reconstruction error: 4.6535\n",
      "Epoch [2400/3000], loss: 0.1526, reconstruction error: 4.6206\n",
      "Epoch [2500/3000], loss: 0.1694, reconstruction error: 4.6217\n",
      "Epoch [2600/3000], loss: 0.1693, reconstruction error: 4.6360\n",
      "Epoch [2700/3000], loss: 0.2470, reconstruction error: 4.6708\n",
      "Epoch [2800/3000], loss: 0.3161, reconstruction error: 4.6595\n",
      "Epoch [2900/3000], loss: 0.2524, reconstruction error: 4.6805\n",
      "Epoch [3000/3000], loss: 0.2692, reconstruction error: 4.6609\n",
      "tensor([[ 0.8786,  0.0071, -0.0190],\n",
      "        [-0.0047,  0.8697,  0.0191]], device='cuda:0')\n",
      "tensor([[ 1.1058,  0.0021, -0.0058],\n",
      "        [ 0.0021,  1.0902,  0.0054],\n",
      "        [-0.0058,  0.0054,  0.5693]], device='cuda:0')\n",
      "tensor([[ 0.5520, -0.0012,  0.5232],\n",
      "        [-0.0192,  0.8734,  0.0156]], device='cuda:0')\n",
      "tensor([[ 7.8701e-01, -1.9961e-03,  7.5409e-02],\n",
      "        [-1.9961e-03,  1.0957e+00,  1.0691e-03],\n",
      "        [ 7.5409e-02,  1.0691e-03,  7.6337e-01]], device='cuda:0')\n",
      "Epsilon: 0.24\n",
      "Epsilon: 0.22\n",
      "tensor(-0.3633, device='cuda:0')\n",
      "tensor(-0.3631, device='cuda:0')\n",
      "tensor(0.2647, device='cuda:0')\n",
      "tensor(0.2649, device='cuda:0')\n",
      "tensor(0.2051, device='cuda:0')\n",
      "tensor(0.2053, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "T = 0.25\n",
    "beta = 1/T\n",
    "\n",
    "n_alpha = 20\n",
    "alpha_range = np.array([1]) # alpha_range = np.linspace(0.1, 1, num = n_alpha, endpoint = True)\n",
    "\n",
    "N = 16000\n",
    "P = 2\n",
    "P_t = 3\n",
    "P_large = 18\n",
    "m_0 = 0.2\n",
    "\n",
    "seed = 693\n",
    "random_number_generator = torch.Generator(device = device)\n",
    "random_number_generator.manual_seed(seed)\n",
    "\n",
    "data_loader_seed = 45\n",
    "data_loader_generator = torch.Generator(device = \"cpu\")\n",
    "data_loader_generator.manual_seed(data_loader_seed)\n",
    "\n",
    "m_A_range = np.zeros(n_alpha)\n",
    "m_B_range = np.zeros((n_alpha, 2))\n",
    "# m_C_range = np.zeros(n_alpha)\n",
    "\n",
    "teacher = RBM(N, P, 1/np.sqrt(N), device = device, random_number_generator = random_number_generator).to(device)\n",
    "\n",
    "student_A = RBM(N, P_t, 1/np.sqrt(N), device = device, random_number_generator = random_number_generator).to(device)\n",
    "student_A.xi[:, 0 : P].copy_(np.sqrt(1 - m_0) * student_A.xi[:, 0 : P] + np.sqrt(m_0) * teacher.xi.detach())\n",
    "\n",
    "student_B = RBM(N, P_t, 1/np.sqrt(N), device = device, random_number_generator = random_number_generator).to(device)\n",
    "student_B.xi[:, 0 : P].copy_(np.sqrt(1 - m_0) * student_B.xi[:, 0 : P] + np.sqrt(m_0) * teacher.xi.detach())\n",
    "student_B.xi[:, P : P_t].copy_(np.sqrt(1 - m_0) * student_B.xi[:, P : P_t] + np.sqrt(m_0) * teacher.xi[:, 0 : 1].detach())\n",
    "\n",
    "# student_C = RBM(N, P_large, 1/np.sqrt(N), device = device, random_number_generator = random_number_generator).to(device)\n",
    "\n",
    "# Sampling converges extremely quickly\n",
    "number_teacher_sampling_steps = 50\n",
    "number_monitored_sampling_steps = 10\n",
    "\n",
    "# decay_rate = 1 is the theoretically correct value in our setting. The other hyperparameters are set heuristically.\n",
    "number_student_sampling_steps = 1\n",
    "learning_rate = 0.005 # 0.5 # 0.005\n",
    "decay_rate = 1\n",
    "momentum = 0.9\n",
    "number_student_training_epochs = 3000 # 30 # 3000\n",
    "number_monitored_training_epochs = 30\n",
    "\n",
    "number_sampling_steps = 50\n",
    "number_annealing_steps = 1000\n",
    "number_annealing_runs = 20\n",
    "number_monitored_sampling_steps = 2\n",
    "number_monitored_annealing_runs = 20\n",
    "seed = 10\n",
    "\n",
    "for i, alpha in enumerate(alpha_range):\n",
    "    M = int(alpha*N)\n",
    "    \n",
    "    data_batch_size = M\n",
    "    \n",
    "    sigma = torch.nn.Parameter(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)), requires_grad = False)\n",
    "    \n",
    "    teacher.sample_visible(sigma, beta, number_teacher_sampling_steps,\n",
    "                           number_monitored_sampling_steps)\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset = sigma, batch_size = data_batch_size, shuffle = True, generator = data_loader_generator)\n",
    "    \n",
    "    # loader : torch.utils.data.DataLoader, beta : float, alpha : float, initial_learning_rate : float,\n",
    "                      # decay_rate : float, momentum : float, number_sampling_steps : int,\n",
    "                      # number_training_epochs : int, number_monitored_training_epochs : int = 0, monitor_sampling : bool = False\n",
    "    \n",
    "    student_A.train_weights(loader, beta, alpha, learning_rate, decay_rate, momentum, number_student_sampling_steps,\n",
    "                            number_student_training_epochs, number_monitored_training_epochs, monitor_sampling = False)\n",
    "    \n",
    "    student_B.train_weights(loader, beta, alpha, learning_rate, decay_rate, momentum, number_student_sampling_steps,\n",
    "                            number_student_training_epochs, number_monitored_training_epochs, monitor_sampling = False)\n",
    "    \n",
    "    # student_C.train_weights(loader, beta, learning_rate, decay_rate, momentum, number_student_sampling_steps,\n",
    "                            # number_student_training_epochs, number_monitored_training_epochs)\n",
    "    \n",
    "    m_A = torch.transpose(teacher.xi, 0, 1) @ student_A.xi\n",
    "    print(m_A)\n",
    "    m_A_range[0] = torch.mean(torch.diagonal(m_A)).item()\n",
    "    \n",
    "    s_A = torch.transpose(student_A.xi, 0, 1) @ student_A.xi\n",
    "    \n",
    "    print(s_A)\n",
    "    \n",
    "    m_B = torch.transpose(teacher.xi, 0, 1) @ student_B.xi\n",
    "    print(m_B)\n",
    "    m_B_range[0, 0] = torch.mean(torch.diagonal(m_B)[1 :]).item()\n",
    "    m_B_range[0, 1] = (m_B[0, 0] + m_B[0, P]).item()/2\n",
    "    \n",
    "    s_B = torch.transpose(student_B.xi, 0, 1) @ student_B.xi\n",
    "    \n",
    "    print(s_B)\n",
    "    \n",
    "    # del sigma\n",
    "    \n",
    "    # outputs = evaluate(teacher, student_A, student_B, beta, M, number_sampling_steps,\n",
    "                       # number_annealing_steps, number_annealing_runs,\n",
    "                       # number_monitored_sampling_steps, number_monitored_annealing_runs, seed)\n",
    "    \n",
    "    # mean_f_difference, std_f_difference, mean_log_likelihood_difference, std_log_likelihood_difference, mean_log_posterior_difference, std_log_posterior_difference = outputs\n",
    "    \n",
    "    f_difference_1, f_difference_2 = analytical_free_energy_difference(student_A, student_B, beta, P_t, device)\n",
    "    print(f_difference_1)\n",
    "    print(f_difference_2)\n",
    "    \n",
    "    log_likelihood_difference_1 = torch.mean(student_A.free_entropy(sigma, beta)) - torch.mean(student_B.free_entropy(sigma, beta)) + f_difference_1\n",
    "    log_likelihood_difference_2 = torch.mean(student_A.free_entropy(sigma, beta)) - torch.mean(student_B.free_entropy(sigma, beta)) + f_difference_2\n",
    "    print(log_likelihood_difference_1)\n",
    "    print(log_likelihood_difference_2)\n",
    "    \n",
    "    log_posterior_difference_1 = log_likelihood_difference_1 + 1/2 * 1/alpha * (torch.sum(student_B.xi**2) - torch.sum(student_A.xi**2))\n",
    "    log_posterior_difference_2 = log_likelihood_difference_2 + 1/2 * 1/alpha * (torch.sum(student_B.xi**2) - torch.sum(student_A.xi**2))\n",
    "    print(log_posterior_difference_1)\n",
    "    print(log_posterior_difference_2)\n",
    "    \n",
    "    # m_C = torch.transpose(teacher.xi, 0, 1) @ student_C.xi\n",
    "    # print(m_C)\n",
    "    # m_C_range[i] = torch.mean(torch.diagonal(m_C)).item()\n",
    "    \n",
    "    # s_C = torch.transpose(student_C.xi, 0, 1) @ student_C.xi\n",
    "    \n",
    "    # print(s_C)\n",
    "    \n",
    "    # Reinitialize weights\n",
    "    student_A.initialize_weights(1/np.sqrt(N))\n",
    "    student_A.xi[:, 0 : P].copy_(np.sqrt(1 - m_0) * student_A.xi[:, 0 : P] + np.sqrt(m_0) * teacher.xi.detach())\n",
    "    \n",
    "    student_B.initialize_weights(1/np.sqrt(N))\n",
    "    student_B.xi[:, 0 : P].copy_(np.sqrt(1 - m_0) * student_B.xi[:, 0 : P] + np.sqrt(m_0) * teacher.xi.detach())\n",
    "    student_B.xi[:, P : P_t].copy_(np.sqrt(1 - m_0) * student_B.xi[:, P : P_t] + np.sqrt(m_0) * teacher.xi[:, 0 : 1].detach())\n",
    "    \n",
    "    # student_C.initialize_weights(1/np.sqrt(N))\n",
    "\n",
    "# del teacher\n",
    "# del student_A\n",
    "# del student_B\n",
    "# del student_C\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f_difference, std_f_difference, mean_log_likelihood_difference, std_log_likelihood_difference, mean_log_posterior_difference, std_log_posterior_difference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step [0/50], free entropy: 14.7838\n",
    "Step [5/50], free entropy: 20.3730\n",
    "Step [10/50], free entropy: 20.3335\n",
    "Step [15/50], free entropy: 20.3922\n",
    "Step [20/50], free entropy: 20.3354\n",
    "Step [25/50], free entropy: 20.2828\n",
    "Step [30/50], free entropy: 20.4022\n",
    "Step [35/50], free entropy: 20.2473\n",
    "Step [40/50], free entropy: 20.3598\n",
    "Step [45/50], free entropy: 20.3599\n",
    "Step [50/50], free entropy: 20.3541\n",
    "Time elapsed: 14.1621.\n",
    "Step [0/50], free entropy: 14.4966\n",
    "Step [5/50], free entropy: 19.8561\n",
    "Step [10/50], free entropy: 19.8336\n",
    "Step [15/50], free entropy: 19.7545\n",
    "Step [20/50], free entropy: 19.7286\n",
    "Step [25/50], free entropy: 19.7091\n",
    "Step [30/50], free entropy: 19.7257\n",
    "Step [35/50], free entropy: 19.6248\n",
    "Step [40/50], free entropy: 19.6958\n",
    "Step [45/50], free entropy: 19.8189\n",
    "Step [50/50], free entropy: 19.7862\n",
    "\n",
    "Step [0/50], free entropy: 7.2735\n",
    "Step [5/50], free entropy: 20.3485\n",
    "Step [10/50], free entropy: 20.3430\n",
    "Step [15/50], free entropy: 20.4679\n",
    "Step [20/50], free entropy: 20.3120\n",
    "Step [25/50], free entropy: 20.3674\n",
    "Step [30/50], free entropy: 20.3619\n",
    "Step [35/50], free entropy: 20.4323\n",
    "Step [40/50], free entropy: 20.4071\n",
    "Step [45/50], free entropy: 20.2980\n",
    "Step [50/50], free entropy: 20.4397\n",
    "Time elapsed: 13.9203.\n",
    "Step [0/50], free entropy: 7.1396\n",
    "Step [5/50], free entropy: 19.6602\n",
    "Step [10/50], free entropy: 19.6745\n",
    "Step [15/50], free entropy: 19.7441\n",
    "Step [20/50], free entropy: 19.8040\n",
    "Step [25/50], free entropy: 19.7264\n",
    "Step [30/50], free entropy: 19.6394\n",
    "Step [35/50], free entropy: 19.7821\n",
    "Step [40/50], free entropy: 19.8453\n",
    "Step [45/50], free entropy: 19.7558\n",
    "Step [50/50], free entropy: 19.7362"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f_difference, std_f_difference, mean_log_likelihood_difference, std_log_likelihood_difference, mean_log_posterior_difference, std_log_posterior_difference"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Time elapsed: 31.2654.\n",
    "Run [1/20], free entropy difference: 21.6648, -11.2731\n",
    "Time elapsed: 31.1055.\n",
    "Run [2/20], free entropy difference: 21.7811, -11.2553\n",
    "Time elapsed: 31.1325.\n",
    "Run [3/20], free entropy difference: 21.8405, -11.2513\n",
    "Time elapsed: 31.0926.\n",
    "Run [4/20], free entropy difference: 21.7791, -11.3205\n",
    "Time elapsed: 31.1191.\n",
    "Run [5/20], free entropy difference: 21.8945, -11.3028\n",
    "Time elapsed: 31.1014.\n",
    "Run [6/20], free entropy difference: 21.7807, -11.2479\n",
    "Time elapsed: 31.1261.\n",
    "Run [7/20], free entropy difference: 21.6568, -11.2586\n",
    "Time elapsed: 31.1222.\n",
    "Run [8/20], free entropy difference: 21.8540, -11.2453\n",
    "Time elapsed: 31.1018.\n",
    "Run [9/20], free entropy difference: 21.7253, -11.2626\n",
    "Time elapsed: 31.1445.\n",
    "Run [10/20], free entropy difference: 21.8555, -11.2715\n",
    "Time elapsed: 31.2914.\n",
    "Run [11/20], free entropy difference: 21.7953, -11.2447\n",
    "Time elapsed: 31.2839.\n",
    "Run [12/20], free entropy difference: 21.7017, -11.2956\n",
    "Time elapsed: 32.7383.\n",
    "Run [13/20], free entropy difference: 21.7766, -11.2345\n",
    "Time elapsed: 31.6647.\n",
    "Run [14/20], free entropy difference: 21.7864, -11.2941\n",
    "Time elapsed: 31.6129.\n",
    "Run [15/20], free entropy difference: 21.6974, -11.2319\n",
    "Time elapsed: 31.6729.\n",
    "Run [16/20], free entropy difference: 21.7597, -11.3030\n",
    "Time elapsed: 31.7252.\n",
    "Run [17/20], free entropy difference: 21.8167, -11.2290\n",
    "Time elapsed: 31.7558.\n",
    "Run [18/20], free entropy difference: 21.7528, -11.2494\n",
    "Time elapsed: 31.7857.\n",
    "Run [19/20], free entropy difference: 21.6550, -11.2431\n",
    "Time elapsed: 31.7560.\n",
    "Run [20/20], free entropy difference: 21.7562, -11.2645"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(teacher, student_1, student_2, M, number_sampling_steps,\n",
    "             number_annealing_steps, number_annealing_runs,\n",
    "             number_monitored_sampling_steps, number_monitored_annealing_steps,\n",
    "             monitor_student_sampling = False):\n",
    "    \n",
    "    device = teacher.training_device\n",
    "    \n",
    "    t_range = torch.linspace(1/number_annealing_steps, 1, number_annealing_steps)\n",
    "    \n",
    "    N = teacher.N\n",
    "    P = teacher.P\n",
    "    P_1 = student_1.P\n",
    "    P_2 = student_2.P\n",
    "    \n",
    "    alpha = M/N\n",
    "    \n",
    "    random_number_generator = teacher.random_number_generator\n",
    "    \n",
    "    mean_f_difference = torch.zeros(2, device = device, requires_grad = False)\n",
    "    var_f_difference = torch.zeros(2, device = device, requires_grad = False)\n",
    "    \n",
    "    mean_log_likelihood_difference = torch.zeros(2, device = device, requires_grad = False)\n",
    "    var_log_likelihood_difference = torch.zeros(2, device = device, requires_grad = False)\n",
    "    \n",
    "    sigma_teacher = torch.nn.Parameter(torch.zeros((M, N), device = device, requires_grad = False))\n",
    "    sigma_student = torch.nn.Parameter(torch.zeros((M, N), device = device, requires_grad = False))\n",
    "    \n",
    "    # number_sampling_steps = 10\n",
    "    # number_monitored_sampling_steps = 10\n",
    "    \n",
    "    # number_annealing_runs = 20\n",
    "    \n",
    "    tau_1 = torch.zeros((M, P_1), device = student_1.training_device)\n",
    "    tau_2 = torch.zeros((M, P_2), device = student_2.training_device)\n",
    "    \n",
    "    for annealing_run in range(number_annealing_runs):\n",
    "        sigma_teacher.copy_(torch.sign(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator))))\n",
    "        sigma_student.copy_(torch.sign(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator))))\n",
    "        \n",
    "        teacher.sample_visible(sigma_teacher, beta, number_sampling_steps,\n",
    "                               number_monitored_sampling_steps, anneal = False)\n",
    "        \n",
    "        for i, (student_A, student_B, tau_A, tau_B) in enumerate([(student_1, student_2, tau_1, tau_2), (student_2, student_1, tau_2, tau_1)]):\n",
    "            sigma_student.copy_(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)))\n",
    "            \n",
    "            student_A.sample_hidden_given_visible(tau_A, sigma_student, beta)\n",
    "            \n",
    "            P_sigma_given_tau = torch.sigmoid(beta * tau_A @ torch.transpose(student_A.xi, 0, 1))\n",
    "            sigma_student.copy_(rademacher(P_sigma_given_tau, generator = random_number_generator))\n",
    "            \n",
    "            f_difference = -student_A.free_entropy(sigma_student, beta)\n",
    "            \n",
    "            for annealing_step, t in enumerate(t_range):\n",
    "                if number_monitored_annealing_steps != 0:\n",
    "                    monitor_annealing_this_epoch = annealing_step % (number_annealing_steps // number_monitored_annealing_steps) == 0\n",
    "                else:\n",
    "                    monitor_annealing_this_epoch = False\n",
    "                f_difference.copy_(f_difference + student_A.free_entropy(sigma_student, (1 - t)*beta) + student_B.free_entropy(sigma_student, t*beta))\n",
    "                \n",
    "                student_A.sample_hidden_given_visible(tau_A, sigma_student, (1 - t)*beta)\n",
    "                student_B.sample_hidden_given_visible(tau_B, sigma_student, t*beta)\n",
    "                \n",
    "                P_sigma_given_tau = torch.sigmoid((1 - t)*beta * tau_A @ torch.transpose(student_A.xi, 0, 1) + t*beta * tau_B @ torch.transpose(student_B.xi, 0, 1))\n",
    "                sigma_student.copy_(rademacher(P_sigma_given_tau, generator = random_number_generator))\n",
    "                \n",
    "                f_difference.copy_(f_difference - student_A.free_entropy(sigma_student, (1 - t)*beta) - student_B.free_entropy(sigma_student, t*beta))\n",
    "                \n",
    "                if monitor_annealing_this_epoch:\n",
    "                    print(\"Run [{}/{}], annealing step [{}/{}], free entropy difference: {:.4f}\".format(annealing_step, number_annealing_steps,\n",
    "                                                                                                        annealing_run, number_annealing_runs, f_difference))\n",
    "            \n",
    "            f_difference.copy_(f_difference + student_B.free_entropy(sigma_student, beta))\n",
    "            \n",
    "            c = torch.max(f_difference)\n",
    "            f_difference = c + torch.log(torch.mean(torch.exp(f_difference - c)))\n",
    "            \n",
    "            log_likelihood_difference = torch.mean(student_A.free_entropy(sigma_teacher, beta)) - torch.mean(student_B.free_entropy(sigma_teacher, beta)) + f_difference\n",
    "            \n",
    "            log_posterior_difference = log_likelihood_difference\n",
    "            \n",
    "            mean_f_difference[i] += (f_difference.detach().item() - mean_f_difference[i]) / (annealing_run + 1)\n",
    "            var_f_difference[i] += (f_difference.detach().item() - mean_f_difference[i])**2 / (annealing_run + 1)\n",
    "            var_f_difference[i] *= annealing_run / (annealing_run + 1)\n",
    "            \n",
    "            mean_log_likelihood_difference[i] += (log_likelihood_difference.detach().item() - mean_log_likelihood_difference[i]) / (annealing_run + 1)\n",
    "            var_log_likelihood_difference[i] += (log_likelihood_difference.detach().item() - mean_log_likelihood_difference[i])**2 / (annealing_run + 1)\n",
    "            var_log_likelihood_difference[i] *= annealing_run / (annealing_run + 1)\n",
    "    \n",
    "    var_f_difference *= number_annealing_run / (number_annealing_run - 1)\n",
    "    var_log_likelihood_difference *= number_annealing_run / (number_annealing_run - 1)\n",
    "    \n",
    "    mean_log_posterior_difference = mean_log_likelihood_difference + 1/alpha * 1/2 * torch.sum(student_B.xi**2) - 1/alpha * 1/2 * torch.sum(student_A.xi**2)\n",
    "    var_log_posterior_difference = var_log_likelihood_difference\n",
    "    \n",
    "    return mean_f_difference, torch.sqrt(var_f_difference), mean_log_likelihood_difference, torch.sqrt(var_log_likelihood_difference), mean_log_posterior_difference, torch.sqrt(var_log_posterior_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_A_range = np.zeros(n_alpha)\n",
    "m_B_range = np.zeros((n_alpha, 2))\n",
    "m_C_range = np.zeros(n_alpha)\n",
    "\n",
    "m_A = torch.transpose(teacher.xi, 0, 1) @ student_A.xi\n",
    "print(m_A)\n",
    "m_A_range[0] = torch.mean(torch.diagonal(m_A)).item()\n",
    "\n",
    "s_A = torch.transpose(student_A.xi, 0, 1) @ student_A.xi\n",
    "\n",
    "print(s_A)\n",
    "\n",
    "m_B = torch.transpose(teacher.xi, 0, 1) @ student_B.xi\n",
    "print(m_B)\n",
    "m_B_range[0, 0] = torch.mean(torch.diagonal(m_B)[1 :]).item()\n",
    "m_B_range[0, 1] = (m_B[0, 0] + m_B[0, P]).item()/2\n",
    "\n",
    "s_B = torch.transpose(student_B.xi, 0, 1) @ student_B.xi\n",
    "\n",
    "print(s_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "t_range = np.linspace(0.001, 1, num = 10000, endpoint = True)\n",
    "\n",
    "N = 16000\n",
    "M = int(2*N)\n",
    "P = 2\n",
    "P_t = 3\n",
    "m_0 = 0.2\n",
    "beta = 4\n",
    "\n",
    "sigma_teacher = torch.nn.Parameter(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)), requires_grad = False)\n",
    "sigma_student = torch.nn.Parameter(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)), requires_grad = False)\n",
    "\n",
    "number_sampling_steps = 100\n",
    "number_monitored_sampling_steps = 100\n",
    "\n",
    "number_repetitions = 20\n",
    "average_probability_ratio = 0\n",
    "\n",
    "tau_A = torch.zeros((M, P_t), device = student_A.training_device)\n",
    "tau_B = torch.zeros((M, P_t), device = student_B.training_device)\n",
    "\n",
    "teacher.sample_visible(sigma_teacher, beta, number_sampling_steps,\n",
    "                       number_monitored_sampling_steps)\n",
    "\n",
    "student_A.sample_hidden_given_visible(tau_A, sigma_student, beta)\n",
    "\n",
    "P_sigma_given_tau = torch.sigmoid(beta * tau_A @ torch.transpose(student_A.xi, 0, 1))\n",
    "sigma_student.copy_(rademacher(P_sigma_given_tau, generator = student_A.random_number_generator))\n",
    "\n",
    "f_difference = -student_A.free_entropy(sigma_student, beta)\n",
    "\n",
    "for t in t_range:\n",
    "    f_difference.copy_(f_difference + student_A.free_entropy(sigma_student, (1 - t)*beta) + student_B.free_entropy(sigma_student, t*beta))\n",
    "    \n",
    "    student_A.sample_hidden_given_visible(tau_A, sigma_student, (1 - t)*beta)\n",
    "    student_B.sample_hidden_given_visible(tau_B, sigma_student, t*beta)\n",
    "    \n",
    "    P_sigma_given_tau = torch.sigmoid((1 - t)*beta * tau_A @ torch.transpose(student_A.xi, 0, 1) + t*beta * tau_B @ torch.transpose(student_B.xi, 0, 1))\n",
    "    sigma_student.copy_(rademacher(P_sigma_given_tau, generator = student_A.random_number_generator))\n",
    "    \n",
    "    f_difference.copy_(f_difference - student_A.free_entropy(sigma_student, (1 - t)*beta) - student_B.free_entropy(sigma_student, t*beta))\n",
    "\n",
    "f_difference.copy_(f_difference + student_B.free_entropy(sigma_student, beta))\n",
    "\n",
    "c = torch.max(f_difference)\n",
    "f_difference = c + torch.log(torch.mean(torch.exp(f_difference - c)))\n",
    "\n",
    "print(f_difference)\n",
    "print(torch.mean(student_A.free_entropy(sigma_teacher, beta)) - torch.mean(student_B.free_entropy(sigma_teacher, beta)) + f_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_difference)\n",
    "print(torch.mean(student_A.free_entropy(sigma_teacher, beta)) - torch.mean(student_B.free_entropy(sigma_teacher, beta)) + f_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "t_range = np.linspace(0.001, 1, num = 10000, endpoint = True)\n",
    "\n",
    "N = 16000\n",
    "M = int(2*N)\n",
    "P = 2\n",
    "P_t = 3\n",
    "m_0 = 0.2\n",
    "beta = 4\n",
    "\n",
    "sigma_teacher = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "sigma_student = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "\n",
    "number_sampling_steps = 100\n",
    "number_monitored_sampling_steps = 100\n",
    "\n",
    "number_repetitions = 20\n",
    "average_probability_ratio = 0\n",
    "\n",
    "sigma_teacher.copy_(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)))\n",
    "sigma_student.copy_(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)))\n",
    "\n",
    "tau_A = torch.zeros((M, P_t), device = student_A.training_device)\n",
    "tau_B = torch.zeros((M, P_t), device = student_B.training_device)\n",
    "\n",
    "teacher.sample_visible(sigma_teacher, beta, number_sampling_steps,\n",
    "                       number_monitored_sampling_steps = 0)\n",
    "\n",
    "student_B.sample_hidden_given_visible(tau_B, sigma_student, beta)\n",
    "\n",
    "P_sigma_given_tau = torch.sigmoid(beta * tau_B @ torch.transpose(student_B.xi, 0, 1))\n",
    "sigma_student.copy_(rademacher(P_sigma_given_tau, generator = student_B.random_number_generator))\n",
    "\n",
    "f_difference = -student_B.free_entropy(sigma_student, beta)\n",
    "\n",
    "for t in t_range:\n",
    "    f_difference.copy_(f_difference + student_B.free_entropy(sigma_student, (1 - t)*beta) + student_A.free_entropy(sigma_student, t*beta))\n",
    "    \n",
    "    student_B.sample_hidden_given_visible(tau_B, sigma_student, (1 - t)*beta)\n",
    "    student_A.sample_hidden_given_visible(tau_A, sigma_student, t*beta)\n",
    "    \n",
    "    P_sigma_given_tau = torch.sigmoid((1 - t)*beta * tau_B @ torch.transpose(student_B.xi, 0, 1) + t*beta * tau_A @ torch.transpose(student_A.xi, 0, 1))\n",
    "    sigma_student.copy_(rademacher(P_sigma_given_tau, generator = student_B.random_number_generator))\n",
    "    \n",
    "    f_difference.copy_(f_difference - student_B.free_entropy(sigma_student, (1 - t)*beta) - student_A.free_entropy(sigma_student, t*beta))\n",
    "\n",
    "f_difference.copy_(f_difference + student_A.free_entropy(sigma_student, beta))\n",
    "\n",
    "c = torch.max(f_difference)\n",
    "f_difference = c + torch.log(torch.mean(torch.exp(f_difference - c)))\n",
    "\n",
    "print(f_difference)\n",
    "print(torch.mean(student_B.free_entropy(sigma_teacher, beta)) - torch.mean(student_A.free_entropy(sigma_teacher, beta)) + f_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta)\n",
    "\n",
    "print(torch.transpose(teacher.xi, 0, 1) @ student_A.xi)\n",
    "\n",
    "print(torch.transpose(teacher.xi, 0, 1) @ student_B.xi)\n",
    "\n",
    "print(torch.mean(student_A.free_entropy(sigma_teacher, beta)))\n",
    "\n",
    "print(torch.mean(student_B.free_entropy(sigma_teacher, beta)))\n",
    "\n",
    "print(torch.mean(student_B.free_entropy(sigma_teacher, beta)) - torch.mean(student_A.free_entropy(sigma_teacher, beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.3735 - 0.4344)\n",
    "\n",
    "print(-0.3735 + 0.4356)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensor(0.4356, device='cuda:0')\n",
    "tensor(0.1032, device='cuda:0')\n",
    "\n",
    "tensor(-0.4344, device='cuda:0')\n",
    "tensor(-0.0608, device='cuda:0')\n",
    "\n",
    "tensor(0.4354, device='cuda:0')\n",
    "tensor(0.0996, device='cuda:0')\n",
    "\n",
    "tensor(-0.4337, device='cuda:0')\n",
    "tensor(-0.0775, device='cuda:0')\n",
    "\n",
    "tensor(0.4389, device='cuda:0')\n",
    "tensor(0.0549, device='cuda:0')\n",
    "\n",
    "tensor(-0.4336, device='cuda:0')\n",
    "tensor(-0.0722, device='cuda:0')\n",
    "\n",
    "tensor(0.4437, device='cuda:0')\n",
    "tensor(0.0937, device='cuda:0')\n",
    "\n",
    "tensor(-0.4296, device='cuda:0')\n",
    "tensor(-0.0689, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "N = 16000\n",
    "M = int(2*N)\n",
    "P = 2\n",
    "P_t = 3\n",
    "m_0 = 0.2\n",
    "beta = 4\n",
    "\n",
    "D = 20\n",
    "\n",
    "sigma_teacher = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "sigma_student = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "\n",
    "number_sampling_steps = 100\n",
    "number_monitored_sampling_steps = 100\n",
    "\n",
    "number_repetitions = 20\n",
    "average_probability_ratio = 0\n",
    "\n",
    "for repetition in range(number_repetitions):\n",
    "    \n",
    "    sigma_teacher.copy_(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)))\n",
    "    sigma_student.copy_(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)))\n",
    "\n",
    "    teacher.sample_visible(sigma_teacher, beta, number_sampling_steps,\n",
    "                           number_monitored_sampling_steps,\n",
    "                           anneal = False, monitor_sampling = False)\n",
    "\n",
    "    student_A.sample_visible(sigma_student, beta, number_sampling_steps,\n",
    "                             number_monitored_sampling_steps,\n",
    "                             anneal = False, monitor_sampling = False)\n",
    "    \n",
    "    f = student_B.free_entropy(sigma_student, beta) - student_A.free_entropy(sigma_student, beta)\n",
    "    c = torch.max(f)\n",
    "    \n",
    "    Z_ratio = torch.exp(c)*torch.mean(torch.exp(f - c))\n",
    "    \n",
    "    f = student_A.free_entropy(sigma_teacher, beta) - student_B.free_entropy(sigma_teacher, beta)\n",
    "    c = torch.max(f)\n",
    "    \n",
    "    probability_ratio = torch.exp(c)*torch.mean(torch.exp(f - c)) * Z_ratio\n",
    "    print(probability_ratio)\n",
    "    \n",
    "    average_probability_ratio += (probability_ratio.detach().item() - average_probability_ratio) / (repetition + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_probability_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "N = 16000\n",
    "M = int(2*N)\n",
    "P = 2\n",
    "P_t = 3\n",
    "m_0 = 0.2\n",
    "beta = 4\n",
    "\n",
    "D = 20\n",
    "\n",
    "sigma_teacher = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "sigma_student = torch.nn.Parameter(torch.zeros((M, N), device = device), requires_grad = False)\n",
    "\n",
    "number_sampling_steps = 100\n",
    "number_monitored_sampling_steps = 100\n",
    "\n",
    "number_repetitions = 20\n",
    "average_probability_ratio = 0\n",
    "\n",
    "for repetition in range(number_repetitions):\n",
    "    \n",
    "    sigma_teacher.copy_(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)))\n",
    "    sigma_student.copy_(torch.sign(torch.randn((M, N), device = device, generator = random_number_generator)))\n",
    "\n",
    "    teacher.sample_visible(sigma_teacher, beta, number_sampling_steps,\n",
    "                           number_monitored_sampling_steps,\n",
    "                           anneal = False, monitor_sampling = False)\n",
    "\n",
    "    student_B.sample_visible(sigma_student, beta, number_sampling_steps,\n",
    "                             number_monitored_sampling_steps,\n",
    "                             anneal = False, monitor_sampling = False)\n",
    "    \n",
    "    f = student_A.free_entropy(sigma_student, beta) - student_B.free_entropy(sigma_student, beta)\n",
    "    c = torch.max(f)\n",
    "    \n",
    "    Z_ratio = torch.exp(c)*torch.mean(torch.exp(f - c))\n",
    "    \n",
    "    f = student_B.free_entropy(sigma_teacher, beta) - student_A.free_entropy(sigma_teacher, beta)\n",
    "    c = torch.max(f)\n",
    "    \n",
    "    probability_ratio = torch.exp(c)*torch.mean(torch.exp(f - c)) * Z_ratio\n",
    "    print(probability_ratio)\n",
    "    \n",
    "    average_probability_ratio += (probability_ratio.detach().item() - average_probability_ratio) / (repetition + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_probability_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
